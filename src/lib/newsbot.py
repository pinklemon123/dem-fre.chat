"""
æ–°é—»æœºå™¨äºº - è®ºå›é›†æˆç‰ˆ
æ•´åˆåŸæœ‰æ–°é—»ç³»ç»Ÿï¼Œè‡ªåŠ¨å‘å¸–åˆ°è®ºå›

åŠŸèƒ½:
1. é›†æˆåŸæœ‰çš„æ–°é—»æŠ“å–å’Œåˆ†æåŠŸèƒ½
2. è‡ªåŠ¨å‘å¸ƒåˆ° Supabase è®ºå›
3. æ”¯æŒå®šæ—¶ä»»åŠ¡
4. æ™ºèƒ½å»é‡å’Œè´¨é‡æ§åˆ¶
"""

import os
import re
import json
import time
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# Supabase é›†æˆ
from supabase import create_client, Client

# é…ç½®
class NewsBot:
    def __init__(self):
        # Supabase é…ç½®
        self.supabase_url = os.getenv("NEXT_PUBLIC_SUPABASE_URL")
        self.supabase_key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
        self.supabase: Optional[Client] = None
        
        # AI API é…ç½®
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
        
        # æœºå™¨äººé…ç½®
        self.bot_user_id = os.getenv("NEWS_BOT_USER_ID")  # éœ€è¦åˆ›å»ºä¸“é—¨çš„æœºå™¨äººè´¦å·
        
        # æ–°é—»æºé…ç½®
        self.news_sources = [
            {
                "name": "BBCä¸­æ–‡",
                "url": "https://www.bbc.com/zhongwen/simp",
                "domain": "bbc.com"
            },
            {
                "name": "å¤®è§†æ–°é—»",
                "url": "https://news.cctv.com/",
                "domain": "cctv.com"
            }
        ]
        
        self.init_supabase()
    
    def init_supabase(self):
        """åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯"""
        if self.supabase_url and self.supabase_key:
            self.supabase = create_client(self.supabase_url, self.supabase_key)
            print("âœ… Supabase è¿æ¥æˆåŠŸ")
        else:
            print("âŒ Supabase é…ç½®ç¼ºå¤±")
    
    def fetch_html(self, url: str) -> Optional[str]:
        """è·å–ç½‘é¡µHTML"""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0 Safari/537.36"
            }
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âŒ è·å–ç½‘é¡µå¤±è´¥ {url}: {e}")
            return None
    
    def extract_news_links(self, html: str, base_url: str, domain: str) -> List[Dict[str, str]]:
        """ä»ç½‘é¡µæå–æ–°é—»é“¾æ¥"""
        soup = BeautifulSoup(html, "html.parser")
        links = []
        
        for link in soup.find_all("a", href=True):
            text = link.get_text(strip=True)
            href = link.get("href")
            
            if not text or len(text) < 15:
                continue
                
            full_url = urljoin(base_url, href)
            
            # åŸºæœ¬è¿‡æ»¤æ¡ä»¶
            if (domain in full_url and 
                len(text) > 15 and 
                text not in links and
                not any(ext in full_url.lower() for ext in ['.jpg', '.png', '.gif', '.mp4'])):
                
                links.append({
                    "title": text,
                    "url": full_url,
                    "source": domain
                })
        
        return links[:20]  # é™åˆ¶æ•°é‡
    
    def extract_article_content(self, url: str) -> Optional[Dict]:
        """æå–æ–‡ç« å†…å®¹"""
        html = self.fetch_html(url)
        if not html:
            return None
            
        soup = BeautifulSoup(html, "html.parser")
        
        # æå–æ ‡é¢˜
        title = None
        for selector in ["h1", "title", '[property="og:title"]']:
            element = soup.select_one(selector)
            if element:
                title = element.get_text(strip=True) or element.get("content")
                if title:
                    break
        
        # æå–æ­£æ–‡
        content = ""
        for selector in ["article", ".article", ".content", ".post", "#content"]:
            element = soup.select_one(selector)
            if element:
                # æ¸…ç†æ— ç”¨æ ‡ç­¾
                for tag in element.select("script,style,nav,header,footer,aside"):
                    tag.decompose()
                
                paragraphs = element.find_all("p")
                content = " ".join([p.get_text(strip=True) for p in paragraphs])
                break
        
        if not content:
            paragraphs = soup.find_all("p")
            content = " ".join([p.get_text(strip=True) for p in paragraphs])
        
        # æå–å°é¢å›¾
        image_url = None
        og_image = soup.select_one('[property="og:image"]')
        if og_image:
            image_url = og_image.get("content")
        
        if not image_url:
            first_img = soup.select_one("img")
            if first_img:
                image_url = urljoin(url, first_img.get("src", ""))
        
        return {
            "title": title,
            "content": content,
            "image_url": image_url,
            "source_url": url
        }
    
    def ai_summarize(self, title: str, content: str, provider: str = "openai") -> Optional[str]:
        """AIæ‘˜è¦ç”Ÿæˆ"""
        if provider == "deepseek":
            api_key = self.deepseek_api_key
            endpoint = "https://api.deepseek.com/v1/chat/completions"
            model = "deepseek-chat"
        else:
            api_key = self.openai_api_key
            endpoint = "https://api.openai.com/v1/chat/completions"
            model = "gpt-4o-mini"
        
        if not api_key:
            print(f"âŒ {provider.upper()} API Key æœªé…ç½®")
            return None
        
        prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–°é—»ç”Ÿæˆç®€æ´çš„æ‘˜è¦ï¼Œè¦æ±‚ï¼š
1. æ§åˆ¶åœ¨150å­—ä»¥å†…
2. çªå‡ºå…³é”®ä¿¡æ¯å’Œå½±å“
3. è¯­è¨€ç®€æ´å®¢è§‚
4. ä¿æŒä¸­æ–‡è¡¨è¾¾

æ ‡é¢˜ï¼š{title}
å†…å®¹ï¼š{content[:2000]}

è¯·ç›´æ¥è¾“å‡ºæ‘˜è¦ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""

        try:
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": model,
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„æ–°é—»ç¼–è¾‘ï¼Œæ“…é•¿ç”Ÿæˆç®€æ´å‡†ç¡®çš„æ–°é—»æ‘˜è¦ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.3,
                "max_tokens": 300
            }
            
            response = requests.post(endpoint, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            return data["choices"][0]["message"]["content"].strip()
            
        except Exception as e:
            print(f"âŒ AIæ‘˜è¦å¤±è´¥: {e}")
            return None
    
    def check_duplicate(self, title: str, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦é‡å¤å‘å¸–"""
        if not self.supabase:
            return False
            
        try:
            # ç”Ÿæˆå†…å®¹æŒ‡çº¹
            content_hash = hashlib.md5(f"{title}{content}".encode()).hexdigest()
            
            # æ£€æŸ¥æœ€è¿‘7å¤©çš„å¸–å­
            week_ago = (datetime.now() - timedelta(days=7)).isoformat()
            
            result = self.supabase.table("posts").select("id").eq("user_id", self.bot_user_id).gte("created_at", week_ago).execute()
            
            # ç®€å•æ£€æŸ¥æ ‡é¢˜ç›¸ä¼¼åº¦
            for post in result.data:
                existing_title = post.get("title", "")
                if self.text_similarity(title, existing_title) > 0.8:
                    return True
            
            return False
        except Exception as e:
            print(f"âŒ é‡å¤æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def text_similarity(self, text1: str, text2: str) -> float:
        """ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—"""
        set1 = set(text1.replace(" ", ""))
        set2 = set(text2.replace(" ", ""))
        
        if not set1 or not set2:
            return 0.0
            
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        return intersection / union if union > 0 else 0.0
    
    def post_to_forum(self, title: str, content: str, image_url: Optional[str] = None) -> bool:
        """å‘å¸ƒåˆ°è®ºå›"""
        if not self.supabase or not self.bot_user_id:
            print("âŒ Supabase æˆ–æœºå™¨äººè´¦å·æœªé…ç½®")
            return False
        
        try:
            post_data = {
                "user_id": self.bot_user_id,
                "title": f"ğŸ“° {title}",
                "content": content,
                "image_url": image_url,
                "image_alt": "æ–°é—»é…å›¾"
            }
            
            result = self.supabase.table("posts").insert(post_data).execute()
            
            if result.data:
                print(f"âœ… æˆåŠŸå‘å¸–: {title}")
                return True
            else:
                print(f"âŒ å‘å¸–å¤±è´¥: {title}")
                return False
                
        except Exception as e:
            print(f"âŒ å‘å¸–å¼‚å¸¸: {e}")
            return False
    
    def run_daily_news(self, max_posts: int = 5) -> Dict:
        """æ‰§è¡Œæ¯æ—¥æ–°é—»æŠ“å–å’Œå‘å¸ƒ"""
        print(f"ğŸ¤– å¼€å§‹æ‰§è¡Œæ¯æ—¥æ–°é—»ä»»åŠ¡ - {datetime.now()}")
        
        results = {
            "total_links": 0,
            "processed": 0,
            "posted": 0,
            "skipped": 0,
            "errors": 0
        }
        
        all_articles = []
        
        # 1. ä»å„æ–°é—»æºæŠ“å–é“¾æ¥
        for source in self.news_sources:
            print(f"ğŸ” æŠ“å–æ–°é—»æº: {source['name']}")
            
            html = self.fetch_html(source["url"])
            if not html:
                continue
                
            links = self.extract_news_links(html, source["url"], source["domain"])
            results["total_links"] += len(links)
            
            # 2. æå–æ–‡ç« å†…å®¹
            for link in links[:10]:  # æ¯ä¸ªæºæœ€å¤šå¤„ç†10ç¯‡
                article = self.extract_article_content(link["url"])
                if article and len(article.get("content", "")) > 200:
                    article["source_name"] = source["name"]
                    all_articles.append(article)
                    results["processed"] += 1
        
        # 3. æŒ‰å†…å®¹é•¿åº¦æ’åºï¼Œé€‰æ‹©è´¨é‡è¾ƒé«˜çš„æ–‡ç« 
        all_articles.sort(key=lambda x: len(x.get("content", "")), reverse=True)
        selected_articles = all_articles[:max_posts * 2]  # å¤šé€‰ä¸€äº›å¤‡ç”¨
        
        # 4. AIæ‘˜è¦å’Œå‘å¸ƒ
        posted_count = 0
        for article in selected_articles:
            if posted_count >= max_posts:
                break
                
            title = article.get("title", "")
            content = article.get("content", "")
            
            # æ£€æŸ¥é‡å¤
            if self.check_duplicate(title, content):
                print(f"â­ï¸  è·³è¿‡é‡å¤å†…å®¹: {title[:30]}...")
                results["skipped"] += 1
                continue
            
            # AIæ‘˜è¦
            summary = self.ai_summarize(title, content)
            if not summary:
                results["errors"] += 1
                continue
            
            # æ„å»ºå¸–å­å†…å®¹
            post_content = f"""ğŸ“° **{article.get('source_name', 'æ–°é—»')}**

{summary}

ğŸ”— [æŸ¥çœ‹åŸæ–‡]({article.get('source_url', '')})

---
*æœ¬å†…å®¹ç”±æ–°é—»æœºå™¨äººè‡ªåŠ¨æŠ“å–æ•´ç†*"""
            
            # å‘å¸ƒåˆ°è®ºå›
            if self.post_to_forum(title, post_content, article.get("image_url")):
                posted_count += 1
                results["posted"] += 1
                time.sleep(2)  # é¿å…é¢‘ç¹å‘å¸–
            else:
                results["errors"] += 1
        
        print(f"âœ… æ¯æ—¥æ–°é—»ä»»åŠ¡å®Œæˆ: {results}")
        return results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    bot = NewsBot()
    
    # æ‰§è¡Œæ¯æ—¥æ–°é—»
    results = bot.run_daily_news(max_posts=3)
    print(f"ä»»åŠ¡ç»“æœ: {results}")