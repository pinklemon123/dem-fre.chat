"""
å¢å¼ºç‰ˆæ–°é—»æœºå™¨äºº - æ”¯æŒè‹±ç¾æ–°é—»æºå’Œè‡ªåŠ¨ç¿»è¯‘
é›†æˆRSSçˆ¬å–ã€AIç¿»è¯‘ã€æ™ºèƒ½åˆ†æåŠŸèƒ½

ç‰¹æ€§:
1. æ”¯æŒBBCã€CNNã€Reutersç­‰è‹±ç¾ä¸»æµåª’ä½“
2. è‡ªåŠ¨ç¿»è¯‘è‹±æ–‡æ–°é—»ä¸ºä¸­æ–‡
3. æ™ºèƒ½å†…å®¹è¿‡æ»¤å’Œè´¨é‡è¯„ä¼°
4. è‡ªåŠ¨å»é‡å’Œåˆ†ç±»
5. å®Œå…¨è‡ªä¸»è¿è¡Œï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®
"""

import os
import re
import json
import time
import hashlib
import logging
import feedparser
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from supabase import create_client, Client

_supabase_client: Optional[Client] = None
logger = logging.getLogger(__name__)


def _mask_secret(value: Optional[str], keep_start: int = 4, keep_end: int = 2) -> str:
    if not value:
        return "<missing>"

    if len(value) <= keep_start + keep_end:
        return "*" * len(value)

    return f"{value[:keep_start]}***{value[-keep_end:]}"


def _summarize_supabase_url(url: str) -> str:
    try:
        parsed = urlparse(url)
        host = parsed.netloc or parsed.path
        if host:
            scheme = parsed.scheme or "https"
            return f"{scheme}://{host}"
    except Exception:
        pass

    return _mask_secret(url, keep_start=8, keep_end=0)


def _get_supabase_client() -> Client:
    """Create (or reuse) the Supabase client using environment variables."""
    global _supabase_client

    if _supabase_client is not None:
        return _supabase_client

    supabase_url = os.getenv("SUPABASE_URL")
    url_source = "SUPABASE_URL" if supabase_url else None
    if not supabase_url:
        supabase_url = os.getenv("NEXT_PUBLIC_SUPABASE_URL")
        if supabase_url:
            url_source = "NEXT_PUBLIC_SUPABASE_URL"

    if not supabase_url:
        raise RuntimeError(
            "Missing Supabase URL. Set SUPABASE_URL or NEXT_PUBLIC_SUPABASE_URL so the Python runtime "
            "can connect to the same instance as Next.js."
        )

    supabase_key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
    key_source = "SUPABASE_SERVICE_ROLE_KEY" if supabase_key else None
    used_fallback_key = False
    if not supabase_key:
        supabase_key = os.getenv("SUPABASE_ANON_KEY")
        if supabase_key:
            key_source = "SUPABASE_ANON_KEY"
            used_fallback_key = True
        else:
            supabase_key = os.getenv("NEXT_PUBLIC_SUPABASE_ANON_KEY")
            if supabase_key:
                key_source = "NEXT_PUBLIC_SUPABASE_ANON_KEY"
                used_fallback_key = True

    if not supabase_key:
        raise RuntimeError(
            "Missing Supabase service role key. Set SUPABASE_SERVICE_ROLE_KEY for the Python runtime "
            "(or provide SUPABASE_ANON_KEY/NEXT_PUBLIC_SUPABASE_ANON_KEY for limited read-only access)."
        )

    masked_key = _mask_secret(supabase_key)
    summarized_url = _summarize_supabase_url(supabase_url)
    print(
        f"ğŸ”§ Supabaseé…ç½®: url={summarized_url} (from {url_source}), key={masked_key} (from {key_source})"
    )
    if used_fallback_key:
        print("âš ï¸ ä½¿ç”¨åŒ¿åå¯†é’¥ä½œä¸ºåå¤‡å‡­è¯ï¼Œå¯èƒ½æ— æ³•æ‰§è¡Œéœ€è¦ Service Role æƒé™çš„æ“ä½œã€‚")

    _supabase_client = create_client(supabase_url, supabase_key)
    return _supabase_client

class EnhancedNewsBot:
    def __init__(self):
        # APIé…ç½®
        self.openai_key = os.getenv("OPENAI_API_KEY")
        self.deepseek_key = os.getenv("DEEPSEEK_API_KEY")
        
        # è‹±ç¾æ–°é—»æºé…ç½® - ä½¿ç”¨RSSç¡®ä¿ç¨³å®šæ€§
        self.news_sources = [
            {
                'id': 'bbc_world',
                'name': 'BBC World News',
                'rss_url': 'http://feeds.bbci.co.uk/news/world/rss.xml',
                'category': 'å›½é™…æ–°é—»',
                'enabled': True
            },
            {
                'id': 'cnn_world',
                'name': 'CNN World',
                'rss_url': 'http://rss.cnn.com/rss/cnn_world.rss',
                'category': 'å›½é™…æ–°é—»',
                'enabled': True
            },
            {
                'id': 'reuters_world',
                'name': 'Reuters World',
                'rss_url': 'https://www.reuters.com/tools/rss/world',
                'category': 'å›½é™…æ–°é—»',
                'enabled': True
            },
            {
                'id': 'guardian_world',
                'name': 'Guardian World',
                'rss_url': 'https://www.theguardian.com/world/rss',
                'category': 'å›½é™…æ–°é—»',
                'enabled': True
            },
            {
                'id': 'ap_news',
                'name': 'Associated Press',
                'rss_url': 'https://apnews.com/index.rss',
                'category': 'å›½é™…æ–°é—»',
                'enabled': True
            }
        ]
        
        # çˆ¬å–é…ç½®
        self.config = {
            'max_articles_per_source': 2,  # æ¯ä¸ªæºæœ€å¤š2ç¯‡
            'total_max_articles': 8,       # æ€»å…±æœ€å¤š8ç¯‡
            'min_content_length': 200,     # æœ€å°å†…å®¹é•¿åº¦
            'quality_threshold': 0.75,     # è´¨é‡é˜ˆå€¼
            'auto_translate': True,        # è‡ªåŠ¨ç¿»è¯‘
            'auto_post': True             # è‡ªåŠ¨å‘å¸ƒ
        }
        
        # HTTPä¼šè¯
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

        # Supabase å®¢æˆ·ç«¯
        self.supabase = _get_supabase_client()
        
    def fetch_rss_articles(self) -> List[Dict]:
        """ä»æ‰€æœ‰RSSæºè·å–æ–°é—»æ–‡ç« """
        all_articles = []
        
        for source in self.news_sources:
            if not source['enabled']:
                continue
                
            try:
                print(f"ğŸ“¡ æ­£åœ¨çˆ¬å– {source['name']}...")
                articles = self._fetch_single_rss(source)
                all_articles.extend(articles)
                time.sleep(2)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                
            except Exception as e:
                print(f"âŒ çˆ¬å–å¤±è´¥ {source['name']}: {e}")
                continue
        
        # æŒ‰è´¨é‡å’Œæ—¶é—´æ’åºï¼Œé€‰æ‹©æœ€å¥½çš„æ–‡ç« 
        all_articles.sort(key=lambda x: (x.get('quality_score', 0), x.get('published_time', 0)), reverse=True)
        return all_articles[:self.config['total_max_articles']]
    
    def _fetch_single_rss(self, source: Dict) -> List[Dict]:
        """ä»å•ä¸ªRSSæºè·å–æ–‡ç« """
        try:
            feed = feedparser.parse(source['rss_url'])
            articles = []
            
            for entry in feed.entries[:self.config['max_articles_per_source']]:
                article = {
                    'title': entry.title,
                    'link': entry.link,
                    'description': getattr(entry, 'description', ''),
                    'published': getattr(entry, 'published', ''),
                    'source_name': source['name'],
                    'source_id': source['id'],
                    'category': source['category'],
                    'language': 'en'
                }
                
                # è·å–å®Œæ•´å†…å®¹
                content = self._extract_article_content(entry.link)
                if content and len(content) >= self.config['min_content_length']:
                    article['content'] = content
                    article['quality_score'] = self._calculate_quality_score(article)
                    articles.append(article)
                    
            return articles
            
        except Exception as e:
            print(f"RSSè§£æå¤±è´¥: {e}")
            return []
    
    def _extract_article_content(self, url: str) -> Optional[str]:
        """æå–æ–‡ç« å®Œæ•´å†…å®¹"""
        try:
            response = self.session.get(url, timeout=30)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style", "nav", "footer", "aside"]):
                script.decompose()
            
            # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                '[data-component="text-block"]',  # BBC
                '.article-content',               # é€šç”¨
                '.story-body',                   # CNN
                '.article-body',                 # Guardian
                '.StandardArticleBody_body'      # Reuters
            ]
            
            content = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = ' '.join([elem.get_text().strip() for elem in elements])
                    break
            
            if not content:
                # åå¤‡æ–¹æ¡ˆï¼šæŸ¥æ‰¾ä¸»è¦æ®µè½
                paragraphs = soup.find_all('p')
                content = ' '.join([p.get_text().strip() for p in paragraphs[:10]])
            
            return self._clean_text(content)
            
        except Exception as e:
            print(f"å†…å®¹æå–å¤±è´¥ {url}: {e}")
            return None
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""
        
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        
        # ç§»é™¤å¹¿å‘Šå’Œç‰ˆæƒä¿¡æ¯
        patterns_to_remove = [
            r'subscribe to.*?newsletter',
            r'follow us on.*?twitter',
            r'copyright.*?\d{4}',
            r'all rights reserved',
            r'terms of use',
            r'privacy policy'
        ]
        
        for pattern in patterns_to_remove:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def _calculate_quality_score(self, article: Dict) -> float:
        """è®¡ç®—æ–‡ç« è´¨é‡åˆ†æ•°"""
        score = 0.5  # åŸºç¡€åˆ†æ•°
        
        content = article.get('content', '')
        title = article.get('title', '')
        
        # å†…å®¹é•¿åº¦è¯„åˆ†
        if len(content) > 500:
            score += 0.2
        if len(content) > 1000:
            score += 0.1
        
        # æ ‡é¢˜è´¨é‡è¯„åˆ†
        if len(title) > 20 and len(title) < 100:
            score += 0.1
        
        # å…³é”®è¯è¯„åˆ†ï¼ˆå›½é™…æ–°é—»ç›¸å…³ï¼‰
        important_keywords = [
            'politics', 'economy', 'technology', 'science', 'climate',
            'international', 'global', 'world', 'breaking'
        ]
        
        content_lower = content.lower()
        for keyword in important_keywords:
            if keyword in content_lower:
                score += 0.05
        
        return min(score, 1.0)  # æœ€é«˜1.0åˆ†
    
    def translate_to_chinese(self, text: str, text_type: str = "content") -> str:
        """ä½¿ç”¨AI APIç¿»è¯‘è‹±æ–‡ä¸ºä¸­æ–‡"""
        if not text or not self.config['auto_translate']:
            return text
        
        try:
            if self.deepseek_key:
                return self._translate_with_deepseek(text, text_type)
            elif self.openai_key:
                return self._translate_with_openai(text, text_type)
            else:
                print("âš ï¸ æœªé…ç½®AI APIå¯†é’¥ï¼Œè·³è¿‡ç¿»è¯‘")
                return text
                
        except Exception as e:
            print(f"ç¿»è¯‘å¤±è´¥: {e}")
            return text
    
    def _translate_with_deepseek(self, text: str, text_type: str) -> str:
        """ä½¿ç”¨DeepSeek APIç¿»è¯‘"""
        try:
            headers = {
                'Authorization': f'Bearer {self.deepseek_key}',
                'Content-Type': 'application/json'
            }
            
            prompt = f"""è¯·å°†ä»¥ä¸‹è‹±æ–‡æ–°é—»{text_type}ç¿»è¯‘æˆè‡ªç„¶æµç•…çš„ä¸­æ–‡ã€‚è¦æ±‚ï¼š
1. ä¿æŒåŸæ–‡çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
2. ä½¿ç”¨ç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯çš„è¯­è¨€
3. ä¿ç•™ä¸“æœ‰åè¯ï¼ˆäººåã€åœ°åã€æœºæ„åï¼‰çš„å¸¸è§ä¸­æ–‡è¯‘å
4. ç¡®ä¿æ–°é—»çš„å®¢è§‚æ€§å’Œä¸“ä¸šæ€§

è‹±æ–‡å†…å®¹ï¼š
{text}

ä¸­æ–‡ç¿»è¯‘ï¼š"""

            data = {
                "model": "deepseek-chat",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.3,
                "max_tokens": 2000
            }
            
            response = requests.post(
                'https://api.deepseek.com/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content'].strip()
            else:
                print(f"DeepSeek APIé”™è¯¯: {response.status_code}")
                return text
                
        except Exception as e:
            print(f"DeepSeekç¿»è¯‘å¤±è´¥: {e}")
            return text
    
    def _translate_with_openai(self, text: str, text_type: str) -> str:
        """ä½¿ç”¨OpenAI APIç¿»è¯‘"""
        try:
            import openai
            openai.api_key = self.openai_key
            
            prompt = f"""å°†ä»¥ä¸‹è‹±æ–‡æ–°é—»{text_type}ç¿»è¯‘æˆä¸­æ–‡ï¼Œè¦æ±‚å‡†ç¡®ã€æµç•…ã€ç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯ï¼š

{text}"""

            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=2000
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"OpenAIç¿»è¯‘å¤±è´¥: {e}")
            return text
    
    def generate_summary(self, content: str) -> str:
        """ç”Ÿæˆæ–°é—»æ‘˜è¦"""
        if not content:
            return ""
        
        try:
            # æå–å‰3æ®µä½œä¸ºæ‘˜è¦
            sentences = content.split('.')
            summary_sentences = sentences[:3]
            summary = '. '.join(summary_sentences).strip()
            
            if len(summary) > 200:
                summary = summary[:200] + "..."
            
            return summary
            
        except Exception as e:
            print(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return content[:200] + "..." if len(content) > 200 else content
    
    def is_duplicate(self, title: str, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤æ–‡ç« ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            # è®¡ç®—å†…å®¹å“ˆå¸Œ
            content_hash = hashlib.md5(title.encode() + content[:500].encode()).hexdigest()
            
            # è¿™é‡Œåº”è¯¥æŸ¥è¯¢æ•°æ®åº“æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            # æš‚æ—¶è¿”å›Falseï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦å®ç°æ•°æ®åº“æŸ¥è¯¢
            return False
            
        except Exception:
            return False
    
    def process_articles(self) -> List[Dict]:
        """å¤„ç†æ‰€æœ‰æ–‡ç« ï¼šçˆ¬å–ã€ç¿»è¯‘ã€åˆ†æ"""
        print("ğŸ¤– æ–°é—»æœºå™¨äººå¼€å§‹å·¥ä½œ...")
        
        # 1. çˆ¬å–RSSæ–‡ç« 
        articles = self.fetch_rss_articles()
        print(f"ğŸ“° è·å–åˆ° {len(articles)} ç¯‡é«˜è´¨é‡æ–‡ç« ")
        
        processed_articles = []
        
        for article in articles:
            try:
                print(f"ğŸ“ å¤„ç†æ–‡ç« : {article['title'][:50]}...")
                
                # 2. è´¨é‡è¿‡æ»¤
                if article.get('quality_score', 0) < self.config['quality_threshold']:
                    print("   âš ï¸ è´¨é‡ä¸è¾¾æ ‡ï¼Œè·³è¿‡")
                    continue
                
                # 3. é‡å¤æ£€æŸ¥
                if self.is_duplicate(article['title'], article.get('content', '')):
                    print("   âš ï¸ é‡å¤å†…å®¹ï¼Œè·³è¿‡")
                    continue
                
                # 4. ç¿»è¯‘æ ‡é¢˜å’Œå†…å®¹
                if self.config['auto_translate']:
                    print("   ğŸŒ ç¿»è¯‘ä¸­...")
                    article['title_zh'] = self.translate_to_chinese(article['title'], "æ ‡é¢˜")
                    article['content_zh'] = self.translate_to_chinese(article.get('content', ''), "å†…å®¹")
                    article['summary_zh'] = self.generate_summary(article['content_zh'])
                else:
                    article['title_zh'] = article['title']
                    article['content_zh'] = article.get('content', '')
                    article['summary_zh'] = self.generate_summary(article['content_zh'])
                
                # 5. æ ¼å¼åŒ–ä¸ºè®ºå›å¸–å­
                formatted_post = self._format_for_forum(article)
                article['forum_post'] = formatted_post
                
                processed_articles.append(article)
                print("   âœ… å¤„ç†å®Œæˆ")
                
                time.sleep(1)  # é¿å…APIé™åˆ¶
                
            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
                continue
        
        print(f"ğŸ‰ å…±å¤„ç†å®Œæˆ {len(processed_articles)} ç¯‡æ–‡ç« ")
        return processed_articles
    
    def _format_for_forum(self, article: Dict) -> Dict:
        """æ ¼å¼åŒ–æ–‡ç« ä¸ºè®ºå›å¸–å­æ ¼å¼"""
        title = article['title_zh']
        content = article['content_zh']
        summary = article['summary_zh']
        source = article['source_name']
        original_link = article['link']
        
        # æ„å»ºè®ºå›å¸–å­å†…å®¹
        forum_content = f"""**{summary}**

{content}

---
**æ¥æº**: {source}  
**åŸæ–‡é“¾æ¥**: {original_link}  
**å‘å¸ƒæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}  

*æœ¬å†…å®¹ç”±æ–°é—»æœºå™¨äººè‡ªåŠ¨æŠ“å–å¹¶ç¿»è¯‘ï¼Œä»…ä¾›å‚è€ƒ*
"""
        
        return {
            'title': title,
            'content': forum_content,
            'category': article['category'],
            'source': source,
            'original_url': original_link,
            'author': 'NewsBot',
            'created_at': datetime.now().isoformat()
        }
    
    def run_once(self) -> Dict:
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„æ–°é—»å¤„ç†æµç¨‹"""
        start_time = time.time()
        try:
            # å¤„ç†æ–‡ç« 
            articles = self.process_articles()
            articles_posted = 0
            bot_user_id = (
                os.getenv("NEWS_BOT_USER_ID")
                or os.getenv("NEXT_PUBLIC_NEWS_BOT_USER_ID")
            )
            if not bot_user_id:
                raise RuntimeError(
                    "ç¼ºå°‘æ–°é—»æœºå™¨äººè´¦å· IDã€‚è¯·åœ¨éƒ¨ç½²ç¯å¢ƒä¸­é…ç½® NEWS_BOT_USER_ID "
                    "ï¼ˆæˆ– NEXT_PUBLIC_NEWS_BOT_USER_IDï¼‰ä»¥æŒ‡å‘å…·æœ‰å‘å¸–æƒé™çš„ Supabase ç”¨æˆ·ã€‚"
                )

            for article in articles:
                post_data = {
                    "title": article['title_zh'],
                    "content": article['forum_post']['content'],
                    "category": article['category'],
                    "source": article['source'],
                    "original_url": article['original_url'],
                    "user_id": bot_user_id,
                    "is_bot_post": True,
                    "created_at": article['forum_post']['created_at']
                }
                try:
                    resp = self.supabase.table("posts").insert(post_data).execute()
                    if hasattr(resp, 'status_code') and resp.status_code in [200, 201]:
                        articles_posted += 1
                except Exception as e:
                    print(f"âŒ å‘å¸–å¤±è´¥: {e}")
            # ç»Ÿè®¡ä¿¡æ¯
            stats = {
                'success': True,
                'articles_processed': len(articles),
                'articles_posted': articles_posted,
                'processing_time': round(time.time() - start_time, 2),
                'timestamp': datetime.now().isoformat(),
                'articles': articles
            }
            print(f"ğŸ“Š è¿è¡Œç»Ÿè®¡:")
            print(f"   å¤„ç†æ–‡ç« : {stats['articles_processed']} ç¯‡")
            print(f"   æˆåŠŸå‘å¸–: {stats['articles_posted']} ç¯‡")
            print(f"   å¤„ç†æ—¶é—´: {stats['processing_time']} ç§’")
            return stats
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'processing_time': round(time.time() - start_time, 2),
                'timestamp': datetime.now().isoformat()
            }

# ä½¿ç”¨ç¤ºä¾‹
def main():
    """ä¸»å‡½æ•° - å¯ä»¥ç›´æ¥è¿è¡Œæµ‹è¯•"""
    bot = EnhancedNewsBot()
    result = bot.run_once()
    
    if result['success']:
        print("âœ… æ–°é—»æœºå™¨äººè¿è¡ŒæˆåŠŸï¼")
        for article in result['articles']:
            print(f"ğŸ“° {article['title_zh']}")
    else:
        print(f"âŒ è¿è¡Œå¤±è´¥: {result['error']}")

if __name__ == "__main__":
    main()